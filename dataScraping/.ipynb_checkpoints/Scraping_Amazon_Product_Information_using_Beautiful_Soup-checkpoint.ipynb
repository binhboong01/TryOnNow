{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j1wZkkQhQz5r",
    "outputId": "9541de4b-5a7a-49e8-ce7e-d5eeca4a4b05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/envs/web_scraping/lib/python3.9/site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/web_scraping/lib/python3.9/site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ykQ_cz9nRHji",
    "outputId": "f51455e3-62d8-49b4-f4e7-e748d4e4c66a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Downloading lxml-5.3.0-cp39-cp39-macosx_10_9_universal2.whl.metadata (3.8 kB)\n",
      "Downloading lxml-5.3.0-cp39-cp39-macosx_10_9_universal2.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "Installing collected packages: lxml\n",
      "Successfully installed lxml-5.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dP_IGNIqRJkE",
    "outputId": "957fee32-3e84-4510-83db-7466d2a001d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/anaconda3/envs/web_scraping/lib/python3.9/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/web_scraping/lib/python3.9/site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/web_scraping/lib/python3.9/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/web_scraping/lib/python3.9/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/web_scraping/lib/python3.9/site-packages (from requests) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.27.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /opt/anaconda3/envs/web_scraping/lib/python3.9/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.28.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /opt/anaconda3/envs/web_scraping/lib/python3.9/site-packages (from selenium) (2024.12.14)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /opt/anaconda3/envs/web_scraping/lib/python3.9/site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /opt/anaconda3/envs/web_scraping/lib/python3.9/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /opt/anaconda3/envs/web_scraping/lib/python3.9/site-packages (from trio~=0.17->selenium) (24.3.0)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/web_scraping/lib/python3.9/site-packages (from trio~=0.17->selenium) (3.10)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/envs/web_scraping/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /opt/anaconda3/envs/web_scraping/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.2.2)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/envs/web_scraping/lib/python3.9/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /opt/anaconda3/envs/web_scraping/lib/python3.9/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Downloading selenium-4.27.1-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading trio-0.28.0-py3-none-any.whl (486 kB)\n",
      "Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Installing collected packages: sortedcontainers, wsproto, outcome, trio, trio-websocket, selenium\n",
      "Successfully installed outcome-1.3.0.post0 selenium-4.27.1 sortedcontainers-2.4.0 trio-0.28.0 trio-websocket-0.11.1 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sQ2-Eo8aRM66"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWU_OKRkSVzY"
   },
   "source": [
    "To generate the right **HEADERS** dictionary for your web scraping use case, follow these steps:\n",
    "\n",
    "1. **Open Developer Tools in your web browser:** Go to the website you want to scrape and open the Developer Tools. Usually this can be done by right-clicking on the page and selecting \"Inspect\" or \"Inspect Element\".\n",
    "2. **Go to the Network Tab:** In the Developer Tools, find and click on the \"Network\" tab.\n",
    "3. **Reload the page:** Refresh the website while the Network tab is open. You will see a list of network requests being made.\n",
    "4. **Find the main request:** Look for the main request that loads the HTML content of the page. This is usually the first or one of the top requests in the list.\n",
    "5. **Inspect the Headers:** Click on the main request to see its details. Find the \"Headers\" tab within the request details.\n",
    "6. **Copy the User-Agent and Accept-Language:** Look for the \"User-Agent\" and \"Accept-Language\" headers and copy their values.\n",
    "7. **Create the HEADERS dictionary:** In your Python code, create a dictionary like the one in your example and paste the copied header values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get('https://www.amazon.com/Best-Sellers-Clothing-Shoes-Jewelry/zgbs/fashion/ref=zg_bs_nav_fashion_0')\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "products = soup.find_all('div', class_='gridItemRoot')\n",
    "print(products)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = soup.find_all('div')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 links:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Amazon Best Sellers URL\n",
    "url = \"https://www.amazon.com/Best-Sellers-Clothing-Shoes-Jewelry/zgbs/fashion/ref=zg_bs_nav_fashion_0\"\n",
    "\n",
    "# Set up headers to mimic a browser\n",
    "# Hiep\n",
    "# headers = {\n",
    "#     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "#     \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "# }\n",
    "\n",
    "#Binh\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "\n",
    "# Send a GET request\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find all item links on the page\n",
    "    links = []\n",
    "    for a_tag in soup.find_all(\"div\", class_=\"gridItemRoot\"):\n",
    "        href = a_tag.get(\"href\")\n",
    "        if href and href.startswith(\"/dp/\"):\n",
    "            full_link = \"https://www.amazon.com\" + href\n",
    "            links.append(full_link)\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_links = list(set(links))\n",
    "\n",
    "    # Print the links\n",
    "    print(\"Found\", len(unique_links), \"links:\")\n",
    "    for link in unique_links:\n",
    "        print(link)\n",
    "else:\n",
    "    print(\"Failed to fetch the page. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Ts7oOowDRdD5"
   },
   "outputs": [],
   "source": [
    "def main(URL):\n",
    "    # opening our output file in append mode\n",
    "    File = open(\"out.csv\", \"a\")\n",
    "\n",
    "    # specifying user agent, You can use other user agents\n",
    "    # available on the internet\n",
    "    HEADERS = {\n",
    "        'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    "        'Accept-Language': 'en-US, en;q=0.5'\n",
    "    }\n",
    "\n",
    "    # Making the HTTP Request\n",
    "    webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "    # Creating the Soup Object containing all data\n",
    "    soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    "\n",
    "    # retrieving product title\n",
    "    try:\n",
    "        # Outer Tag Object\n",
    "        title = soup.find(\"span\", attrs={\n",
    "            'id': 'productTitle'\n",
    "        })\n",
    "\n",
    "        # Inner NavigableString Object\n",
    "        title_value = title.string\n",
    "\n",
    "        # Title as a string value\n",
    "        title_string = title_value.strip().replace(',', '')\n",
    "\n",
    "    except AttributeError:\n",
    "        title_string = \"NA\"\n",
    "    print(\"product Title = \", title_string)\n",
    "\n",
    "    # saving the title in the file\n",
    "    File.write(f\"{title_string},\")\n",
    "\n",
    "    # Binh's comment:\n",
    "    # This id priceblock might not be practical since I always see NA whichever links I scrap.\n",
    "    # And seems like the price on Amazon will always change. So a price tracker could be a good approach for this\n",
    "    # Please refer to the link to understand how to implement a price tracker and integrate it in this code\n",
    "    # https://www.youtube.com/watch?v=qukjS96clB8\n",
    "    \n",
    "    # retrieving price\n",
    "    try:\n",
    "        price = soup.find(\"span\", attrs={\n",
    "            'id': 'priceblock_ourprice'\n",
    "        }).string.strip().replace(',', '')\n",
    "        # we are omitting unnecessary spaces\n",
    "        # and commas form our string\n",
    "    except AttributeError:\n",
    "        price = \"NA\"\n",
    "    print(\"Products price = \", price)\n",
    "\n",
    "    # saving\n",
    "    File.write(f\"{price},\")\n",
    "\n",
    "    # retrieving product rating\n",
    "    try:\n",
    "        rating = soup.find(\"i\", attrs={\n",
    "                           'class': 'a-icon a-icon-star a-star-4-5'\n",
    "        }).string.strip().replace(',', '')\n",
    "\n",
    "    except AttributeError:\n",
    "\n",
    "        try:\n",
    "            rating = soup.find(\"span\", attrs={\n",
    "                'class': 'a-icon-alt'\n",
    "            }).string.strip().replace(',', '')\n",
    "        except:\n",
    "            rating = \"NA\"\n",
    "    print(\"Overall rating = \", rating)\n",
    "\n",
    "    File.write(f\"{rating},\")\n",
    "\n",
    "    try:\n",
    "        review_count = soup.find(\"span\", attrs={\n",
    "            'id': 'acrCustomerReviewText'\n",
    "        }).string.strip().replace(',', '')\n",
    "\n",
    "    except AttributeError:\n",
    "        review_count = \"NA\"\n",
    "    print(\"Total reviews = \", review_count)\n",
    "    File.write(f\"{review_count},\")\n",
    "\n",
    "    # Binh's comment: This one might also need a tracker too\n",
    "    # print availablility status\n",
    "    try:\n",
    "        available = soup.find(\"div\", attrs={'id': 'availability'})\n",
    "        available = available.find(\"span\").string.strip().replace(',', '')\n",
    "\n",
    "    except AttributeError:\n",
    "        available = \"NA\"\n",
    "    print(\"Availability = \", available)\n",
    "\n",
    "    # saving the availability and closing the line\n",
    "    File.write(f\"{available},\\n\")\n",
    "\n",
    "    ####################################################################################################################\n",
    "    ### CAN WE ALSO GET OTHER INFO?                                                                                  ###\n",
    "    ### COLOR, SIZE, PRODUCT DETAILS (FABRIC, CARE INSTRUCTIONS, ORIGIN, ETC), ABOUT THIS ITEM (PRODUCT DESCRIPTION) ###\n",
    "    ####################################################################################################################\n",
    "\n",
    "    # closing the file\n",
    "    File.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BIsFP8iKT_bj",
    "outputId": "6042e1eb-8f0f-420a-e380-9c5db1efe171"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product Title =  Dremel - 3D40-FLX-EDU DigiLab 3D40 Flex 3D Printer w/Extra Supplies 30 Lesson Plans Professional Development Course Flexible Build Plate Automated 9-Point Leveling PC & MAC OS Chromebook iPad Compatible\n",
      "Products price =  NA\n",
      "Overall rating =  3.7 out of 5 stars\n",
      "Total reviews =  46 ratings\n",
      "Availability =  NA\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(\"https://www.amazon.com/Dremel-Education-Accessories-Professional-Development/dp/B07KZ8XNDT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "e7OrRzPNUDVo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product Title =  Original Prusa MK4 3D Printer kit Removable Print Sheets Beginner-Friendly 3D Printer DYI Kit Fun to Assemble Automatic Calibration Filament Sample Included Print Size 9.84×8.3×8.6 in.\n",
      "Products price =  NA\n",
      "Overall rating =  4.3 out of 5 stars\n",
      "Total reviews =  32 ratings\n",
      "Availability =  Only 1 left in stock (more on the way).\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(\"https://www.amazon.com/Original-Removable-Beginner-Friendly-Calibration-9-84%C3%978-3%C3%978-6/dp/B0CKSW74GX/?_encoding=UTF8&pd_rd_w=sHaZz&content-id=amzn1.sym.255b3518-6e7f-495c-8611-30a58648072e%3Aamzn1.symc.a68f4ca3-28dc-4388-a2cf-24672c480d8f&pf_rd_p=255b3518-6e7f-495c-8611-30a58648072e&pf_rd_r=2GQR5F8HV4R3H1T55724&pd_rd_wg=jhZtJ&pd_rd_r=fa1ae611-71fb-486b-a43d-50a4fabe5a29&ref_=pd_hp_d_atf_ci_mcx_mr_ca_hp_atf_d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title_value:         THE GYM PEOPLE Thick High Waist Yoga Pants with Pockets, Tummy Control Workout Running Yoga Leggings for Women       \n",
      "product Title =  THE GYM PEOPLE Thick High Waist Yoga Pants with Pockets Tummy Control Workout Running Yoga Leggings for Women\n",
      "Products price =  NA\n",
      "Overall rating =  4.4 out of 5 stars\n",
      "Total reviews =  56390 ratings\n",
      "Availability =  In Stock\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(\"https://www.amazon.com/GYM-PEOPLE-Pockets-Control-Leggings/dp/B07HQM6NH8/ref=zg_bs_g_fashion_d_sccl_1/132-2939833-0120902?psc=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
